{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "-CjuCswR10UU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "import sqlite3\n",
    "from re import search\n",
    "import datetime\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-012d5286050f>:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  from tqdm._tqdm_notebook import tqdm_notebook\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly_express as px\n",
    "import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LFDMOJqU10UW"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KRoIMG9n10UX"
   },
   "outputs": [],
   "source": [
    "# file_path1 = \"Gross disposable income-Table 1.csv\"\n",
    "# file_path2 = \"dft-road-casualty-statistics-accident-2020.csv\"\n",
    "# file_path3 = 'education.csv'\n",
    "\n",
    "file_path1 = '/Users/jasleensandhu/Desktop/DBL/DC2/GDHI - uk/Gross disposable income-Table 1.csv'\n",
    "file_path2 = '/Users/jasleensandhu/Desktop/Vis/dft-road-casualty-statistics-accident-2020.csv'\n",
    "file_path3 = 'education.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter out data with the dataframe\n",
    "### These are Avon coordinates\n",
    "lang_lats = pd.read_csv('unique_long_lat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_lats = list(pd.Series(lang_lats['0']).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_lats = [lang_lats[i].strip('()') for i in range(0,len(lang_lats))]\n",
    "lang_lats = [i.strip(',') for i in lang_lats]\n",
    "lang_lats = [i.split(', ') for i in lang_lats]\n",
    "lang_lats = [tuple(coords) for coords in lang_lats]\n",
    "lang_lats = [float(i) for coords in lang_lats for i in coords]\n",
    "# [coords for sublist in lst for coords in sublist]\n",
    "it = iter(lang_lats)\n",
    "lang_lats = [*zip(it, it)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "avon_df = pd.read_csv('avon_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "avon_df.columns = ['index','Crime_ID', 'Month', 'Reported_by', 'Falls_within', 'Longitude', 'Latitude', \n",
    "                   'Location', 'LSOA_code', 'LSOA_name', 'Crime_type', 'Last_outcome_category', 'Context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ireh72k010UY"
   },
   "outputs": [],
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tF3VViFz10UY"
   },
   "outputs": [],
   "source": [
    "## Income per region\n",
    "\n",
    "# education should have education level and region level four for evey region\n",
    "# find resource\n",
    "# Gross income index of region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "ZJa56SOB10UZ"
   },
   "outputs": [],
   "source": [
    "gdhi = pd.read_csv(file_path1, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdhi_2019 = pd.read_csv('GDHI_2019.csv', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdhi_2019 = gdhi_2019.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>level_2</th>\n",
       "      <th>level_3</th>\n",
       "      <th>level_4</th>\n",
       "      <th>level_5</th>\n",
       "      <th>level_6</th>\n",
       "      <th>level_7</th>\n",
       "      <th>Table 1: Gross disposable household income by UK and constituent countries and regions, UK, 2019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>South \\nWest</td>\n",
       "      <td>5,624,696</td>\n",
       "      <td>21,222</td>\n",
       "      <td>2.4</td>\n",
       "      <td>99.0</td>\n",
       "      <td>119,370</td>\n",
       "      <td>2.9</td>\n",
       "      <td>8.4</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         level_0    level_1 level_2 level_3 level_4  level_5 level_6 level_7  \\\n",
       "11  South \\nWest  5,624,696  21,222     2.4    99.0  119,370     2.9     8.4   \n",
       "\n",
       "   Table 1: Gross disposable household income by UK and constituent countries and regions, UK, 2019  \n",
       "11                                                8.3                                                "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdhi_2019.head(3)\n",
    "gdhi_2019['level_0'].unique()\n",
    "gdhi_2019[gdhi_2019['level_0'] == 'South \\nWest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "b_h3XV7m10Ub"
   },
   "outputs": [],
   "source": [
    "all_gdhi = []\n",
    "for i in range(1, len(gdhi)):\n",
    "    all_gdhi.append(gdhi.iloc[i][0].split(\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TYKicFe910Ub"
   },
   "outputs": [],
   "source": [
    "df_gdhi = pd.DataFrame(all_gdhi[1:],columns=all_gdhi[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Ld5u4OhJmOSF"
   },
   "outputs": [],
   "source": [
    "df_gdhi['1997'] = pd.to_numeric(df_gdhi['1997'])\n",
    "df_gdhi['1998'] = pd.to_numeric(df_gdhi['1998'])\n",
    "df_gdhi['1999'] = pd.to_numeric(df_gdhi['1999'])\n",
    "df_gdhi['2000'] = pd.to_numeric(df_gdhi['2000'])\n",
    "df_gdhi['2001'] = pd.to_numeric(df_gdhi['2001'])\n",
    "df_gdhi['2002'] = pd.to_numeric(df_gdhi['2002'])\n",
    "df_gdhi['2003'] = pd.to_numeric(df_gdhi['2003'])\n",
    "df_gdhi['2004'] = pd.to_numeric(df_gdhi['2004'])\n",
    "df_gdhi['2005'] = pd.to_numeric(df_gdhi['2005'])\n",
    "df_gdhi['2006'] = pd.to_numeric(df_gdhi['2006'])\n",
    "df_gdhi['2007'] = pd.to_numeric(df_gdhi['2007'])\n",
    "df_gdhi['2008'] = pd.to_numeric(df_gdhi['2008'])\n",
    "df_gdhi['2009'] = pd.to_numeric(df_gdhi['2009'])\n",
    "df_gdhi['2010'] = pd.to_numeric(df_gdhi['2010'])\n",
    "df_gdhi['2011'] = pd.to_numeric(df_gdhi['2011'])\n",
    "df_gdhi['2012'] = pd.to_numeric(df_gdhi['2012'])\n",
    "df_gdhi['2013'] = pd.to_numeric(df_gdhi['2013'])\n",
    "df_gdhi['2014'] = pd.to_numeric(df_gdhi['2014'])\n",
    "df_gdhi['2015'] = pd.to_numeric(df_gdhi['2015'])\n",
    "df_gdhi['2016'] = pd.to_numeric(df_gdhi['2016'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "g9RUxJDYmw-B"
   },
   "outputs": [],
   "source": [
    "df_gdhi = df_gdhi.groupby(['Region', 'LA name']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdhi = df_gdhi.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdhi.drop(['1997', '1998', '1999', '2000', '2001', '2002', '2003',\n",
    "                 '2004', '2005', '2006', '2007', '2008',\n",
    "                 '2009', '2010', '2011', '2012', '2013'], axis=1, inplace=True)\n",
    "\n",
    "# df.drop('column_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "south_west = gdhi[gdhi['Region'] == 'South West'] # income per house hold per district "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "income = pd.read_csv('/Users/jasleensandhu/Desktop/DBL/DC2/regionalgrossdisposablehouseholdincomecityregions/Table 1-Table 1.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_income = []\n",
    "for i in range(1, len(income)):\n",
    "    all_income.append(income.iloc[i][0].split(\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income = pd.DataFrame(all_income[1:],columns=range(0, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income.to_csv('df_income.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substring = \"Avon\"\n",
    "# substring_2 = 'Somerset'\n",
    "# substring_3 = 'Constabulary'\n",
    "# district_lst = [i for i in df_gdhi.reset_index()['LA name'] if search(substring, i) ] + [i for i in df_gdhi.reset_index()['LA name'] if search(substring_2, i) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Filter based on district_lst\n",
    "\n",
    "# gdhi = gdhi[gdhi['LA name'].isin(district_lst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RimnCPpN10Ud"
   },
   "outputs": [],
   "source": [
    "## Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "qLqBetJX10Ud"
   },
   "outputs": [],
   "source": [
    "edu = pd.read_csv(file_path3, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "l2nnBK_X10Ue"
   },
   "outputs": [],
   "source": [
    "all_edu = []\n",
    "for i in range(1, len(edu)):\n",
    "    all_edu.append(edu.iloc[i][0].split(\";\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "qN8hvIVz10Uf"
   },
   "outputs": [],
   "source": [
    "df_edu = pd.DataFrame(all_edu[1:],columns=all_edu[2])\n",
    "\n",
    "df_edu_without = df_edu.loc[2:15]\n",
    "df_edu_GCSE = df_edu.loc[20:33]\n",
    "df_edu_Alevel = df_edu.loc[38:51]\n",
    "df_edu_degree = df_edu.loc[56:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "u7GgZhT1ds_v"
   },
   "outputs": [],
   "source": [
    "df_edu_without.set_index('Date', inplace = True)\n",
    "df_edu_GCSE.set_index('Date', inplace = True)\n",
    "df_edu_Alevel.set_index('Date', inplace = True)\n",
    "df_edu_degree.set_index('Date', inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "6Pcc1gdxezHJ"
   },
   "outputs": [],
   "source": [
    "df_edu_without = df_edu_without.T\n",
    "df_edu_GCSE = df_edu_GCSE.T\n",
    "df_edu_Alevel = df_edu_Alevel.T\n",
    "df_edu_degree = df_edu_degree.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "OFF-9LojnjLQ"
   },
   "outputs": [],
   "source": [
    "df_education = pd.concat([df_edu_without,df_edu_GCSE, df_edu_Alevel, df_edu_degree], keys=['without', 'GCSE', 'GCE A-level', 'degree'],\n",
    "          names=['Eduction', 'Region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_education = df_education.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The dataframe that only focuses on South West and this one is needed\n",
    "south_west_education = (df_education[df_education['Region'] == 'South West'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "south_west_education.to_csv('south_west_education.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "hZxJKbit10Ug"
   },
   "outputs": [],
   "source": [
    "### Road analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCLmH21C10Uh",
    "outputId": "db8cbbf7-c24a-4ff0-aef7-ba2435955be6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (0,2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df_roads = pd.read_csv(file_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2020])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_roads['accident_year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eke Code\n",
    "df_roads[df_roads['lsoa_of_accident_location'] == 'E01014399']\n",
    "df_roads['date'] = df_roads['date'].astype(str)\n",
    "df_roads['month'] = df_roads['date'].str[3:]\n",
    "accidents_grouped = df_roads.groupby([\"local_authority_district\", \"month\"]).count()\n",
    "\n",
    "df_accidents = accidents_grouped[\"accident_index\"].unstack() #with NaN values\n",
    "\n",
    "df_accidents.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "zY9xCReJZkLt",
    "outputId": "015e5503-fc5c-470c-90e3-662a6710a393"
   },
   "outputs": [],
   "source": [
    "# The columns to to be retirved: accident_year, longitude, latitude, accident_severity, \n",
    "# weather_conditions, urban_or_rural_area, lsoa_of_accident_location\n",
    "\n",
    "### Retrrive relevant columns - relevant df needed for building big df\n",
    "\n",
    "roads_df = df_roads[['accident_index','accident_year', 'longitude', 'latitude', 'accident_severity', \n",
    "                    'weather_conditions', 'urban_or_rural_area', 'lsoa_of_accident_location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2020])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roads_df['accident_year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_lat_road = (list(zip(roads_df['longitude'], roads_df['latitude'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_lat_road = (pd.Series(long_lat_road).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_lat_road = list(long_lat_road)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter the relevant rows for getting district for Avon\n",
    "\n",
    "# filtered_road = list(itertools.filterfalse(lambda x:x not in long_lat_road,lang_lats))\n",
    "# range approach\n",
    "\n",
    "sorted_lats = sorted([lang_lats[i][0] for i in range(0, len(lang_lats))])\n",
    "sorted_lats_range = list(drange(int(sorted_lats[0]), int(sorted_lats[-1]), '0.000001'))\n",
    "\n",
    "sorted_long = sorted([lang_lats[i][1] for i in range(0, len(lang_lats))])\n",
    "sorted_long_range = list(drange(int(sorted_long[0]), int(sorted_long[-1]), '0.000001'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats_road = [i[0] for i in long_lat_road]\n",
    "long_road = [i[1] for i in long_lat_road]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_road_lats = list(itertools.filterfalse(lambda x:x not in sorted_lats,lats_road))\n",
    "filtered_road_long = list(itertools.filterfalse(lambda x:x not in sorted_long,long_road))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use the longitude coordinates to get more location\n",
    "# filtered_road_long\n",
    "roads = (roads_df).drop_duplicates(subset='accident_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "roads = roads[['accident_year','accident_severity','weather_conditions','urban_or_rural_area','lsoa_of_accident_location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2020])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "roads.to_csv('roads.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04 is not present in the range.\n"
     ]
    }
   ],
   "source": [
    "range_1 = (list(drange(int(sorted_lats[0]), int(sorted_lats[-1]), '0.05')))\n",
    "number = 0.04\n",
    " \n",
    "if number in range_1 :\n",
    "    print(number, 'is present in the range.')\n",
    "else :\n",
    "    print(number, 'is not present in the range.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.001,\n",
       " 0.002,\n",
       " 0.003,\n",
       " 0.004,\n",
       " 0.005,\n",
       " 0.006,\n",
       " 0.007,\n",
       " 0.008,\n",
       " 0.009,\n",
       " 0.01,\n",
       " 0.011,\n",
       " 0.012,\n",
       " 0.013,\n",
       " 0.014,\n",
       " 0.015,\n",
       " 0.016,\n",
       " 0.017,\n",
       " 0.018,\n",
       " 0.019,\n",
       " 0.02,\n",
       " 0.021,\n",
       " 0.022,\n",
       " 0.023,\n",
       " 0.024,\n",
       " 0.025,\n",
       " 0.026,\n",
       " 0.027,\n",
       " 0.028,\n",
       " 0.029,\n",
       " 0.03,\n",
       " 0.031,\n",
       " 0.032,\n",
       " 0.033,\n",
       " 0.034,\n",
       " 0.035,\n",
       " 0.036,\n",
       " 0.037,\n",
       " 0.038,\n",
       " 0.039,\n",
       " 0.04,\n",
       " 0.041,\n",
       " 0.042,\n",
       " 0.043,\n",
       " 0.044,\n",
       " 0.045,\n",
       " 0.046,\n",
       " 0.047,\n",
       " 0.048,\n",
       " 0.049,\n",
       " 0.05,\n",
       " 0.051,\n",
       " 0.052,\n",
       " 0.053,\n",
       " 0.054,\n",
       " 0.055,\n",
       " 0.056,\n",
       " 0.057,\n",
       " 0.058,\n",
       " 0.059,\n",
       " 0.06,\n",
       " 0.061,\n",
       " 0.062,\n",
       " 0.063,\n",
       " 0.064,\n",
       " 0.065,\n",
       " 0.066,\n",
       " 0.067,\n",
       " 0.068,\n",
       " 0.069,\n",
       " 0.07,\n",
       " 0.071,\n",
       " 0.072,\n",
       " 0.073,\n",
       " 0.074,\n",
       " 0.075,\n",
       " 0.076,\n",
       " 0.077,\n",
       " 0.078,\n",
       " 0.079,\n",
       " 0.08,\n",
       " 0.081,\n",
       " 0.082,\n",
       " 0.083,\n",
       " 0.084,\n",
       " 0.085,\n",
       " 0.086,\n",
       " 0.087,\n",
       " 0.088,\n",
       " 0.089,\n",
       " 0.09,\n",
       " 0.091,\n",
       " 0.092,\n",
       " 0.093,\n",
       " 0.094,\n",
       " 0.095,\n",
       " 0.096,\n",
       " 0.097,\n",
       " 0.098,\n",
       " 0.099,\n",
       " 0.1,\n",
       " 0.101,\n",
       " 0.102,\n",
       " 0.103,\n",
       " 0.104,\n",
       " 0.105,\n",
       " 0.106,\n",
       " 0.107,\n",
       " 0.108,\n",
       " 0.109,\n",
       " 0.11,\n",
       " 0.111,\n",
       " 0.112,\n",
       " 0.113,\n",
       " 0.114,\n",
       " 0.115,\n",
       " 0.116,\n",
       " 0.117,\n",
       " 0.118,\n",
       " 0.119,\n",
       " 0.12,\n",
       " 0.121,\n",
       " 0.122,\n",
       " 0.123,\n",
       " 0.124,\n",
       " 0.125,\n",
       " 0.126,\n",
       " 0.127,\n",
       " 0.128,\n",
       " 0.129,\n",
       " 0.13,\n",
       " 0.131,\n",
       " 0.132,\n",
       " 0.133,\n",
       " 0.134,\n",
       " 0.135,\n",
       " 0.136,\n",
       " 0.137,\n",
       " 0.138,\n",
       " 0.139,\n",
       " 0.14,\n",
       " 0.141,\n",
       " 0.142,\n",
       " 0.143,\n",
       " 0.144,\n",
       " 0.145,\n",
       " 0.146,\n",
       " 0.147,\n",
       " 0.148,\n",
       " 0.149,\n",
       " 0.15,\n",
       " 0.151,\n",
       " 0.152,\n",
       " 0.153,\n",
       " 0.154,\n",
       " 0.155,\n",
       " 0.156,\n",
       " 0.157,\n",
       " 0.158,\n",
       " 0.159,\n",
       " 0.16,\n",
       " 0.161,\n",
       " 0.162,\n",
       " 0.163,\n",
       " 0.164,\n",
       " 0.165,\n",
       " 0.166,\n",
       " 0.167,\n",
       " 0.168,\n",
       " 0.169,\n",
       " 0.17,\n",
       " 0.171,\n",
       " 0.172,\n",
       " 0.173,\n",
       " 0.174,\n",
       " 0.175,\n",
       " 0.176,\n",
       " 0.177,\n",
       " 0.178,\n",
       " 0.179,\n",
       " 0.18,\n",
       " 0.181,\n",
       " 0.182,\n",
       " 0.183,\n",
       " 0.184,\n",
       " 0.185,\n",
       " 0.186,\n",
       " 0.187,\n",
       " 0.188,\n",
       " 0.189,\n",
       " 0.19,\n",
       " 0.191,\n",
       " 0.192,\n",
       " 0.193,\n",
       " 0.194,\n",
       " 0.195,\n",
       " 0.196,\n",
       " 0.197,\n",
       " 0.198,\n",
       " 0.199,\n",
       " 0.2,\n",
       " 0.201,\n",
       " 0.202,\n",
       " 0.203,\n",
       " 0.204,\n",
       " 0.205,\n",
       " 0.206,\n",
       " 0.207,\n",
       " 0.208,\n",
       " 0.209,\n",
       " 0.21,\n",
       " 0.211,\n",
       " 0.212,\n",
       " 0.213,\n",
       " 0.214,\n",
       " 0.215,\n",
       " 0.216,\n",
       " 0.217,\n",
       " 0.218,\n",
       " 0.219,\n",
       " 0.22,\n",
       " 0.221,\n",
       " 0.222,\n",
       " 0.223,\n",
       " 0.224,\n",
       " 0.225,\n",
       " 0.226,\n",
       " 0.227,\n",
       " 0.228,\n",
       " 0.229,\n",
       " 0.23,\n",
       " 0.231,\n",
       " 0.232,\n",
       " 0.233,\n",
       " 0.234,\n",
       " 0.235,\n",
       " 0.236,\n",
       " 0.237,\n",
       " 0.238,\n",
       " 0.239,\n",
       " 0.24,\n",
       " 0.241,\n",
       " 0.242,\n",
       " 0.243,\n",
       " 0.244,\n",
       " 0.245,\n",
       " 0.246,\n",
       " 0.247,\n",
       " 0.248,\n",
       " 0.249,\n",
       " 0.25,\n",
       " 0.251,\n",
       " 0.252,\n",
       " 0.253,\n",
       " 0.254,\n",
       " 0.255,\n",
       " 0.256,\n",
       " 0.257,\n",
       " 0.258,\n",
       " 0.259,\n",
       " 0.26,\n",
       " 0.261,\n",
       " 0.262,\n",
       " 0.263,\n",
       " 0.264,\n",
       " 0.265,\n",
       " 0.266,\n",
       " 0.267,\n",
       " 0.268,\n",
       " 0.269,\n",
       " 0.27,\n",
       " 0.271,\n",
       " 0.272,\n",
       " 0.273,\n",
       " 0.274,\n",
       " 0.275,\n",
       " 0.276,\n",
       " 0.277,\n",
       " 0.278,\n",
       " 0.279,\n",
       " 0.28,\n",
       " 0.281,\n",
       " 0.282,\n",
       " 0.283,\n",
       " 0.284,\n",
       " 0.285,\n",
       " 0.286,\n",
       " 0.287,\n",
       " 0.288,\n",
       " 0.289,\n",
       " 0.29,\n",
       " 0.291,\n",
       " 0.292,\n",
       " 0.293,\n",
       " 0.294,\n",
       " 0.295,\n",
       " 0.296,\n",
       " 0.297,\n",
       " 0.298,\n",
       " 0.299,\n",
       " 0.3,\n",
       " 0.301,\n",
       " 0.302,\n",
       " 0.303,\n",
       " 0.304,\n",
       " 0.305,\n",
       " 0.306,\n",
       " 0.307,\n",
       " 0.308,\n",
       " 0.309,\n",
       " 0.31,\n",
       " 0.311,\n",
       " 0.312,\n",
       " 0.313,\n",
       " 0.314,\n",
       " 0.315,\n",
       " 0.316,\n",
       " 0.317,\n",
       " 0.318,\n",
       " 0.319,\n",
       " 0.32,\n",
       " 0.321,\n",
       " 0.322,\n",
       " 0.323,\n",
       " 0.324,\n",
       " 0.325,\n",
       " 0.326,\n",
       " 0.327,\n",
       " 0.328,\n",
       " 0.329,\n",
       " 0.33,\n",
       " 0.331,\n",
       " 0.332,\n",
       " 0.333,\n",
       " 0.334,\n",
       " 0.335,\n",
       " 0.336,\n",
       " 0.337,\n",
       " 0.338,\n",
       " 0.339,\n",
       " 0.34,\n",
       " 0.341,\n",
       " 0.342,\n",
       " 0.343,\n",
       " 0.344,\n",
       " 0.345,\n",
       " 0.346,\n",
       " 0.347,\n",
       " 0.348,\n",
       " 0.349,\n",
       " 0.35,\n",
       " 0.351,\n",
       " 0.352,\n",
       " 0.353,\n",
       " 0.354,\n",
       " 0.355,\n",
       " 0.356,\n",
       " 0.357,\n",
       " 0.358,\n",
       " 0.359,\n",
       " 0.36,\n",
       " 0.361,\n",
       " 0.362,\n",
       " 0.363,\n",
       " 0.364,\n",
       " 0.365,\n",
       " 0.366,\n",
       " 0.367,\n",
       " 0.368,\n",
       " 0.369,\n",
       " 0.37,\n",
       " 0.371,\n",
       " 0.372,\n",
       " 0.373,\n",
       " 0.374,\n",
       " 0.375,\n",
       " 0.376,\n",
       " 0.377,\n",
       " 0.378,\n",
       " 0.379,\n",
       " 0.38,\n",
       " 0.381,\n",
       " 0.382,\n",
       " 0.383,\n",
       " 0.384,\n",
       " 0.385,\n",
       " 0.386,\n",
       " 0.387,\n",
       " 0.388,\n",
       " 0.389,\n",
       " 0.39,\n",
       " 0.391,\n",
       " 0.392,\n",
       " 0.393,\n",
       " 0.394,\n",
       " 0.395,\n",
       " 0.396,\n",
       " 0.397,\n",
       " 0.398,\n",
       " 0.399,\n",
       " 0.4,\n",
       " 0.401,\n",
       " 0.402,\n",
       " 0.403,\n",
       " 0.404,\n",
       " 0.405,\n",
       " 0.406,\n",
       " 0.407,\n",
       " 0.408,\n",
       " 0.409,\n",
       " 0.41,\n",
       " 0.411,\n",
       " 0.412,\n",
       " 0.413,\n",
       " 0.414,\n",
       " 0.415,\n",
       " 0.416,\n",
       " 0.417,\n",
       " 0.418,\n",
       " 0.419,\n",
       " 0.42,\n",
       " 0.421,\n",
       " 0.422,\n",
       " 0.423,\n",
       " 0.424,\n",
       " 0.425,\n",
       " 0.426,\n",
       " 0.427,\n",
       " 0.428,\n",
       " 0.429,\n",
       " 0.43,\n",
       " 0.431,\n",
       " 0.432,\n",
       " 0.433,\n",
       " 0.434,\n",
       " 0.435,\n",
       " 0.436,\n",
       " 0.437,\n",
       " 0.438,\n",
       " 0.439,\n",
       " 0.44,\n",
       " 0.441,\n",
       " 0.442,\n",
       " 0.443,\n",
       " 0.444,\n",
       " 0.445,\n",
       " 0.446,\n",
       " 0.447,\n",
       " 0.448,\n",
       " 0.449,\n",
       " 0.45,\n",
       " 0.451,\n",
       " 0.452,\n",
       " 0.453,\n",
       " 0.454,\n",
       " 0.455,\n",
       " 0.456,\n",
       " 0.457,\n",
       " 0.458,\n",
       " 0.459,\n",
       " 0.46,\n",
       " 0.461,\n",
       " 0.462,\n",
       " 0.463,\n",
       " 0.464,\n",
       " 0.465,\n",
       " 0.466,\n",
       " 0.467,\n",
       " 0.468,\n",
       " 0.469,\n",
       " 0.47,\n",
       " 0.471,\n",
       " 0.472,\n",
       " 0.473,\n",
       " 0.474,\n",
       " 0.475,\n",
       " 0.476,\n",
       " 0.477,\n",
       " 0.478,\n",
       " 0.479,\n",
       " 0.48,\n",
       " 0.481,\n",
       " 0.482,\n",
       " 0.483,\n",
       " 0.484,\n",
       " 0.485,\n",
       " 0.486,\n",
       " 0.487,\n",
       " 0.488,\n",
       " 0.489,\n",
       " 0.49,\n",
       " 0.491,\n",
       " 0.492,\n",
       " 0.493,\n",
       " 0.494,\n",
       " 0.495,\n",
       " 0.496,\n",
       " 0.497,\n",
       " 0.498,\n",
       " 0.499]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x / 10.0 for x in range(5, 50, 15)]\n",
    "list(drange(0, 0.5, '0.001'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = lang_lats[0].strip(',')\n",
    "# lst = string.split(', ') \n",
    "# [float(i) for i in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal\n",
    "def drange(x, y, jump):\n",
    "    while x < y:\n",
    "        yield float(x)\n",
    "        x += decimal.Decimal(jump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = lang_lats[:2]\n",
    "lst = [i.strip(',') for i in string]\n",
    "lst = [i.split(', ') for i in lst]\n",
    "lst = [tuple(coords) for coords in lst]\n",
    "lst = [float(i) for coords in lst for i in coords]\n",
    "# [coords for sublist in lst for coords in sublist]\n",
    "it = iter(lst)\n",
    "lst = [*zip(it, it)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-2.511761, 51.409966), (-2.515816, 51.408717)]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To Do\n",
    "# Get One Big Dataframe with all different features\n",
    "# Build Lasso\n",
    "# Decide on what to predict - perhaps use \"falls within\" and use this to predict by moving to continous rather categorical\n",
    "\n",
    "# filter df with Avon and Somerset Constabulary\n",
    "\n",
    "## get clusters that is low in prediction so we can do compartive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'policeDatabaseOutcomesV5.db'\n",
    "file2 = 'policeDatabaseStopAndSearchV5.db'\n",
    "file3 = 'policeDatabaseV5.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_query(file_name, query_type, name_table):\n",
    "\n",
    "    dbfile = file_name\n",
    "    \n",
    "    # Create a SQL connection to our SQLite database\n",
    "    con = sqlite3.connect(dbfile) \n",
    "\n",
    "\n",
    "    if query_type == \"limit\":\n",
    "        \n",
    "        data = pd.DataFrame(con.execute(\"SELECT * FROM {}\".format(name_table))) #after {} LIMIT 10000\n",
    "        # gets the first 1000 files from database\n",
    "    \n",
    "        return data\n",
    "\n",
    "    else:\n",
    "        cur = con.cursor()\n",
    "        cur.execute('SELECT COUNT(*) from {}'.format(name_table))\n",
    "        cur_result = cur.fetchone()      \n",
    "        \n",
    "        return cur_result\n",
    "    \n",
    "def col_names(file_name, table_name):\n",
    "    connection = sqlite3.connect(file_name)\n",
    "    cursor = connection.execute('select * from {}'.format(table_name))\n",
    "    names = list(map(lambda x: x[0], cursor.description))\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcome = set_query(file1, 'limit', 'outcomes')\n",
    "col_names_list = col_names(file1, 'outcomes')\n",
    "df_outcome.columns = col_names_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime = set_query(file3, 'limit', 'crimes')\n",
    "col_names_list = col_names(file3, 'crimes')\n",
    "df_crime.columns = col_names_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop = set_query(file2, 'limit', 'stopsearch')\n",
    "col_names_list = col_names(file2, 'stopsearch')\n",
    "df_stop.columns = col_names_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopsearch = df_stop.drop(['Part_of_a_policing_operation', 'Policing_operation', 'Gender', 'Age_range',\n",
    "                       'Self-defined_ethnicity', 'Officer-defined_ethnicity', 'Legislation',\n",
    "                       'Object_of_search', 'Outcome_linked_to_object_of_search', \n",
    "                           'Removal_of_more_than_just_outer_clothing'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopsearch.drop(['Type'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopsearch['Date'] = pd.to_datetime(stopsearch['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopsearch['Year'] = [i.year for i in stopsearch['Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter Nan values\n",
    "\n",
    "stopsearch = stopsearch[stopsearch['Latitude'].notna() | stopsearch['Longitude'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>month</th>\n",
       "      <th>01/2020</th>\n",
       "      <th>02/2020</th>\n",
       "      <th>03/2020</th>\n",
       "      <th>04/2020</th>\n",
       "      <th>05/2020</th>\n",
       "      <th>06/2020</th>\n",
       "      <th>07/2020</th>\n",
       "      <th>08/2020</th>\n",
       "      <th>09/2020</th>\n",
       "      <th>10/2020</th>\n",
       "      <th>11/2020</th>\n",
       "      <th>12/2020</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_authority_district</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>65.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "month                     01/2020  02/2020  03/2020  04/2020  05/2020  \\\n",
       "local_authority_district                                                \n",
       "-1                           65.0     60.0     32.0     20.0     40.0   \n",
       " 1                          100.0    103.0     64.0     26.0     69.0   \n",
       " 2                           61.0     73.0     54.0     20.0     33.0   \n",
       "\n",
       "month                     06/2020  07/2020  08/2020  09/2020  10/2020  \\\n",
       "local_authority_district                                                \n",
       "-1                           61.0     49.0     72.0     77.0    163.0   \n",
       " 1                           64.0     97.0    111.0    124.0     99.0   \n",
       " 2                           46.0     60.0     59.0     66.0     57.0   \n",
       "\n",
       "month                     11/2020  12/2020  \n",
       "local_authority_district                    \n",
       "-1                          157.0    195.0  \n",
       " 1                           74.0     98.0  \n",
       " 2                           41.0     43.0  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_accidents.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app = Nominatim(user_agent=\"tutorial\")\n",
    "# location = [app.geocode(\"{}, England\".format(i)).raw for i in south_west['Region']]\n",
    "# # print raw data\n",
    "# print(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['50.6794784', '50.7594784', '-1.9167712', '-1.8367712']\n"
     ]
    }
   ],
   "source": [
    "app = Nominatim(user_agent=\"tutorial\")\n",
    "location = app.geocode(\"Bournemouth, England\").raw['boundingbox']\n",
    "# print raw data\n",
    "print(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemgetter(1, 3, 2, 5)(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_lats_str = list(pd.Series(lang_lats_str).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_states(i):\n",
    "    return locator.reverse([i]).raw['address']['state_district']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_lats_s = lang_lats_str[:3] + [\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['50.705809, -1.49944', '50.937776, -1.465688', '50.898832, -1.406438', '0']"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_lats_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'South East England'"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_states(lang_lats_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang_lats_1 = lang_lats_str[:3]\n",
    "# location_lst_1 = [locator.reverse(i) for i in lang_lats_1]\n",
    "states_last = [i.raw['address']['state_district'] for i in location_lst_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['South East England', 'South East England', 'South East England']"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locator = Nominatim(user_agent= \"myGeocoder\")\n",
    "# location_lst = [locator.reverse(i) for i in lang_lats_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entry is a\n",
      "Oops! <class 'ValueError'> occurred.\n",
      "Next entry.\n",
      "\n",
      "The entry is 0\n",
      "Oops! <class 'ZeroDivisionError'> occurred.\n",
      "Next entry.\n",
      "\n",
      "The entry is 2\n",
      "The reciprocal of 2 is 0.5\n"
     ]
    }
   ],
   "source": [
    "# import module sys to get the type of exception\n",
    "import sys\n",
    "\n",
    "randomList = ['a', 0, 2]\n",
    "\n",
    "for entry in randomList:\n",
    "    try:\n",
    "        print(\"The entry is\", entry)\n",
    "        r = 1/int(entry)\n",
    "        break\n",
    "    except:\n",
    "        print(\"Oops!\", sys.exc_info()[0], \"occurred.\")\n",
    "        print(\"Next entry.\")\n",
    "        print()\n",
    "print(\"The reciprocal of\", entry, \"is\", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting State District\n",
    "state_district = [i.raw['address']['state_district'] for i in location_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'place_id': 57939459,\n",
       " 'licence': 'Data © OpenStreetMap contributors, ODbL 1.0. https://osm.org/copyright',\n",
       " 'osm_type': 'node',\n",
       " 'osm_id': 5171155156,\n",
       " 'lat': '50.7058309',\n",
       " 'lon': '-1.4992911',\n",
       " 'display_name': \"Spence Willard, St James' Square, Yarmouth, Isle of Wight, South East England, England, PO41 0NP, United Kingdom\",\n",
       " 'address': {'office': 'Spence Willard',\n",
       "  'road': \"St James' Square\",\n",
       "  'town': 'Yarmouth',\n",
       "  'county': 'Isle of Wight',\n",
       "  'state_district': 'South East England',\n",
       "  'state': 'England',\n",
       "  'postcode': 'PO41 0NP',\n",
       "  'country': 'United Kingdom',\n",
       "  'country_code': 'gb'},\n",
       " 'boundingbox': ['50.7057809', '50.7058809', '-1.4993411', '-1.4992411']}"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retriving Data linking to latitude and longitude\n",
    "\n",
    "locator = Nominatim(user_agent= \"myGeocoder\")\n",
    "coordinates = \"53.480837, -2.244914\"\n",
    "coords = stopsearch['Latitude'].iloc[0], stopsearch['Longitude'].iloc[0]\n",
    "location = locator.reverse(coords)\n",
    "location.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'South East England'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# location.address\n",
    "\n",
    "(location.raw['address']['state_district'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making df Avon and Somerset Constabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avon_frame(file_name, year):\n",
    "    dbfile = file_name\n",
    "\n",
    "    # Create a SQL connection to our SQLite database\n",
    "    con = sqlite3.connect(dbfile) \n",
    "\n",
    "    cur = con.cursor()\n",
    "\n",
    "    query = \"\"\" SELECT\n",
    "        \n",
    "         Crime_ID,\n",
    "         Month,\n",
    "         Reported_by,\n",
    "         Falls_within,\n",
    "         Longitude,\n",
    "         Latitude,\n",
    "         Location,\n",
    "         LSOA_code,\n",
    "         LSOA_name,\n",
    "         Crime_type,\n",
    "         Last_outcome_category,\n",
    "         Context\n",
    "    FROM\n",
    "       crimes\n",
    "    WHERE\n",
    "       Falls_within LIKE '%{}%'\n",
    "    \"\"\".format(year)   \n",
    "\n",
    "    result = (cur.execute(query))\n",
    "        \n",
    "    result_df = pd.DataFrame(result.fetchall())\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avon__frame(file_name, year):\n",
    "    dbfile = file_name\n",
    "\n",
    "    # Create a SQL connection to our SQLite database\n",
    "    con = sqlite3.connect(dbfile) \n",
    "\n",
    "    cur = con.cursor()\n",
    "\n",
    "    query = \"\"\" SELECT *\n",
    "    FROM\n",
    "       crimes\n",
    "    WHERE\n",
    "       Falls_within LIKE '%{}%'\n",
    "    \"\"\".format(year)   \n",
    "\n",
    "    result = (cur.execute(query))\n",
    "        \n",
    "    result_df = pd.DataFrame(result.fetchall())\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2014 = time_frame(file3, 'crimes', '2014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avon_df = avon_frame(file3, 'Avon and Somerset Constabulary') \n",
    "avon__df = avon__frame(file3, 'Avon and Somerset Constabulary') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert above dataframe into csv\n",
    "avon__df.to_csv('avon_data.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_frame(file_name, name_table, year):\n",
    "    dbfile = file_name\n",
    "\n",
    "    # Create a SQL connection to our SQLite database\n",
    "    con = sqlite3.connect(dbfile) \n",
    "\n",
    "    cur = con.cursor()\n",
    "\n",
    "    query = \"\"\" SELECT\n",
    "         Crime_ID,\n",
    "         Month,\n",
    "         Reported_by,\n",
    "         Falls_within,\n",
    "         Longitude,\n",
    "         Latitude,\n",
    "         Location,\n",
    "         LSOA_code,\n",
    "         LSOA_name,\n",
    "         Crime_type,\n",
    "         Last_outcome_category,\n",
    "         Context\n",
    "    FROM\n",
    "       {}\n",
    "    WHERE\n",
    "       Falls_within LIKE '%{}%'\n",
    "    \"\"\".format(name_table, year)\n",
    "\n",
    "    result = (cur.execute(query))\n",
    "        \n",
    "    result_df = pd.DataFrame(result.fetchall())\n",
    "\n",
    "    return result_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "aggregation_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
